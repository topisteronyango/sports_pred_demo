# -*- coding: utf-8 -*-
"""Sports_PredictionAssign_AI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16CBvHV7Bzw3e2mHIPuESrec3GFoxKE8T
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np  
import pandas as pd

df_train = pd.read_csv('/content/drive/My Drive/SportsPredictionAssignment/players_22.csv')

#Getting some information on the dataset
df_train.info()

#Shape of the dataset. It has 19239 rows and 110 columns
df_train.shape

# see which datatypes are there and some might need changing.
df_train.dtypes

# Looking at the overall rating column, it shows that the lowest rating is 47, the max is 93 and the average is 65.77
df_train.describe()

df_train.head()

# Drop irrelevant columns
df_train.drop(['sofifa_id', 'player_url', 'short_name', 'long_name', 'dob', 'real_face', 'nation_jersey_number', 'nation_position'], axis=1, inplace=True)

# Checking for missing values
# Returning the percentage of missing values in each column
for c in df_train.columns:
    print('{} -   {}%'.format(c,round((np.mean(df_train[c].isnull()))*100)))

#lets handle the above missing values
col = []
for c in df_train.columns:
    missing_values=np.mean(df_train[c].isnull())* 100
    if missing_values > 60:
        print('{} - {}%'.format(c, round(missing_values)))
        col.append(c)

print("\n We need to drop these columns: \n \n", col)

#Drop the columns with missing values
df_train.drop(columns=['club_loaned_from', 'nation_team_id', 'player_tags', 'goalkeeping_speed', 'nation_logo_url'], inplace=True)

# Drop rows with missing data
df_train.dropna(inplace=True)

df_train = df_train.loc[:, df_train.notna().all()]

# Extract preferred foot
df_train['preferred_foot'] = df_train['preferred_foot'].apply(lambda x: 1 if x == 'Right' else 0)

# Calculate overall potential
df_train['overall_potential'] = df_train['overall'] + df_train['potential']

# Calculate correlation coefficients
corr_matrix = df_train.corr()
corr_matrix

corr_matrix['overall'].sort_values(ascending=False)

#Drop less correlated features
df_train.drop(columns=['goalkeeping_positioning','goalkeeping_reflexes','goalkeeping_diving',
                         'goalkeeping_handling','goalkeeping_kicking',
                         'preferred_foot','nationality_id','league_level','club_jersey_number','club_team_id'])

# Select top features
feature_subset = df_train[['age', 'potential', 'value_eur', 'wage_eur', 'international_reputation', 'overall_potential']]

## creating a new df with only numerical dtypes
new_df = df_train[['potential', 'potential', 'value_eur', 'wage_eur', 
            'age',  'height_cm', 'weight_kg', 
            'pace',
          'shooting',
          'passing',
          'dribbling',
          'defending',
          'physic',
          'attacking_crossing',
          'attacking_finishing',
          'attacking_heading_accuracy',
          'attacking_short_passing',
          'attacking_volleys',
          'skill_dribbling',
          'skill_curve',
          'skill_fk_accuracy',
          'skill_long_passing',
          'skill_ball_control',
          'movement_acceleration',
          'movement_sprint_speed',
          'movement_agility',
          'movement_reactions',
          'movement_balance',
          'power_shot_power',
          'power_jumping',
          'power_stamina',
          'power_strength',
          'power_long_shots',
          'mentality_aggression',
          'mentality_interceptions',
          'mentality_positioning',
          'mentality_vision',
          'mentality_penalties',
          'mentality_composure',
          'defending_marking_awareness',
          'defending_standing_tackle',
          'defending_sliding_tackle',
          'goalkeeping_diving',
          'goalkeeping_handling',
          'goalkeeping_kicking',
          'goalkeeping_positioning',
          'goalkeeping_reflexes',
            ]].copy()

new_df.dtypes

# changing all the dtypes to float64
new_df = new_df.astype(np.float64)

#dropping all the null values
new_df = new_df.notna()
new_df.isnull().any()

#training the model
labels = df_train['overall']
from sklearn.ensemble import RandomForestRegressor
forest = RandomForestRegressor(random_state=42)
forest.fit(new_df, labels)

#Assess the model on the testing set
from sklearn.metrics import mean_squared_error
forestPred = forest.predict(new_df)
forest_mse = mean_squared_error(labels, forestPred)
forest_rmse = np.sqrt(forest_mse)
forest_mse, forest_rmse

# Tune the model by adding the parameters
forest_new = RandomForestRegressor(random_state=42, max_features=1)
forest_new.fit(new_df, labels)

#checking the perfomance of the above model
newForestPred = forest_new.predict(new_df)
newForest_mse = mean_squared_error(labels, newForestPred)
newForest_rmse = np.sqrt(newForest_mse)
newForest_mse, newForest_rmse

# Testing the model using a new dataset
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the new dataset
df_test= pd.read_csv('/content/drive/My Drive/SportsPredictionAssignment/players_21.csv')

new_data= df_test[['overall', 'potential', 'value_eur', 'wage_eur', 
            'age',  'height_cm', 'weight_kg', 
            'pace',
          'shooting',
          'passing',
          'dribbling',
          'defending',
          'physic',
          'attacking_crossing',
          'attacking_finishing',
          'attacking_heading_accuracy',
          'attacking_short_passing',
          'attacking_volleys',
          'skill_dribbling',
          'skill_curve',
          'skill_fk_accuracy',
          'skill_long_passing',
          'skill_ball_control',
          'movement_acceleration',
          'movement_sprint_speed',
          'movement_agility',
          'movement_reactions',
          'movement_balance',
          'power_shot_power',
          'power_jumping',
          'power_stamina',
          'power_strength',
          'power_long_shots',
          'mentality_aggression',
          'mentality_interceptions',
          'mentality_positioning',
          'mentality_vision',
          'mentality_penalties',
          'mentality_composure',
          'defending_marking_awareness',
          'defending_standing_tackle',
          'defending_sliding_tackle',
          'goalkeeping_diving',
          'goalkeeping_handling',
          'goalkeeping_kicking',
          'goalkeeping_positioning',
          'goalkeeping_reflexes',
            ]].copy()

new_data.dropna(inplace=True)

# Preprocess the data
X = new_data.drop('overall', axis=1)
y = new_data['overall']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

prediction_linear=lin_reg.predict(X_test)
prediction_linear

#accuracy of testing prediction i.e R-squared
accuracy = lin_reg.score(X_test, y_test)
print(accuracy)

#accuracy of training dataset
lin_reg.score(X_train,y_train)

"""6. Deploy the model on a simple web page using either (Heroku, Streamlite or Flask) and upload a link to the video that shows how the model performs on the web page/site. [5]

"""

import pickle
from sklearn.linear_model._base import _preprocess_data


# Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))
print(model.predict([[2, 9, 6, 2, 9, 6, 2, 9, 6, 2, 9, 6, 2, 9, 6, 2, 9, 6, 2, 9, 6, 2, 9, 6, 2, 9, 6, 2, 9, 6, 2, 9, 6, 2, 9, 3, 6, 2, 9, 6, 2, 9, 6, 72, 9, 6, ]]))

